"""
Evaluation script for measuring mean squared error.

Prerequisite:
For local testing
- Completed training job
- download train model into local directory: "/opt/ml/processing/model/model.tar.gz"
- Test data at "/opt/ml/processing/test/test.csv"
- Output json metric must match sagemaker pipeline 'Condition': regression_metrics.rmse.value"

The script will:
- load test data generated by processor: "/opt/ml/processing/test/test.csv"
- perform predictions
- output evaluation report in json format: "/opt/ml/processing/evaluation/evaluation.json"

"""
import argparse
import json
import logging
import os
import pathlib
import tarfile

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset

logger = logging.getLogger()
logger.setLevel(logging.INFO)
logger.addHandler(logging.StreamHandler())


class Net(nn.Module):
    def __init__(self, input_size=7, output_size=1):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(input_size, 512)
        self.fc2 = nn.Linear(512, output_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x


class CustomDataset(Dataset):
    def __init__(self, filename):
        """Create cosware dataset. Assume single channel 0."""

        df = pd.read_csv(filename)
        self.X = torch.tensor(df.iloc[:, 1:].values).float()
        self.y = torch.tensor(df.iloc[:, 0:1].values).float()

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

    def __len__(self):
        return len(self.y)


def model_fn(model_dir):
    """Load model."""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = Net()
    if torch.cuda.device_count() > 1:
        print("Gpu count: {}".format(torch.cuda.device_count()))
        model = nn.DataParallel(model)

    with open(os.path.join(model_dir, "model.pth"), "rb") as f:
        model.load_state_dict(torch.load(f))
    return model.to(device)


def test(model, test_loader, device):
    """Run evaluation over test data."""
    model.eval()
    test_loss = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.mse_loss(output, target, reduction="sum").item()  # sum up batch loss

    mse = test_loss / len(test_loader.dataset)
    rmse = np.sqrt(mse)
    logger.info("Test set: Loss(MSE): {:.2f}, RMSE: {:.2f}".format(mse, rmse))
    return rmse


def get_data(filename, args):
    """Return dataloader."""
    test_dataset = CustomDataset(filename)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True)
    return test_loader


def arg_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--batch-size",
        type=int,
        default=64,
        help="evaluate batch size",
    )
    args = parser.parse_args()
    print(args)

    return args


if __name__ == "__main__":
    logger.info("Starting evaluation.")

    args = arg_parser()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"Device: {device}")

    model_path = "/opt/ml/processing/model/model.tar.gz"
    with tarfile.open(model_path) as tar:
        tar.extractall(path=".")

    logger.info("Loading model.")
    model = model_fn("./")

    logger.info("Reading test data.")
    test_path = "/opt/ml/processing/test/test.csv"
    test_loader = get_data(test_path, args)

    logger.info("Performing predictions against test data.")
    rmse = test(model, test_loader, device)
    report_dict = {
        "regression_metrics": {
            "rmse": {"value": rmse},
        },
    }

    output_dir = "/opt/ml/processing/evaluation"
    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)

    logger.info("Writing out evaluation report with rmse: %f", rmse)
    evaluation_path = f"{output_dir}/evaluation.json"
    with open(evaluation_path, "w") as f:
        f.write(json.dumps(report_dict))
